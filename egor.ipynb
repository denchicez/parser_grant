{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "egor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjC956IDNY4V",
        "outputId": "51049cf8-70d4-4a4f-c156-1827e0bebd59"
      },
      "source": [
        "!sudo apt update\r\n",
        "!sudo apt install python3-pip\r\n",
        "!pip3 install beautifulsoup4\r\n",
        "!pip3 install requests\r\n",
        "!pip3 install vk\r\n",
        "!pip3 install validators\r\n",
        "!pip3 install joblib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [1 \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [506 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [237 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,816 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,136 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,699 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.3 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,372 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [266 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,244 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.8 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 11.6 MB in 4s (3,277 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "56 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pkg-resources python3-secretstorage python3-setuptools python3-six\n",
            "  python3-wheel python3-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
            "  python3-six python3-wheel python3-xdg\n",
            "0 upgraded, 15 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 2,882 kB of archives.\n",
            "After this operation, 8,886 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.4 [1,653 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.4 [220 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.4 [114 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-xdg all 0.25-4ubuntu1 [31.4 kB]\n",
            "Fetched 2,882 kB in 2s (1,662 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 15.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../01-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../02-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../03-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../04-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../05-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../06-python3-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../07-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../08-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../09-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../10-python3-pip_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../11-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../12-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../13-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../14-python3-xdg_0.25-4ubuntu1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Collecting vk\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/fd/698ba8b622ba57d7d936aaf7bf8256fec4e7e2e1c2f3b36fc04381df5281/vk-2.0.2.tar.gz\n",
            "Requirement already satisfied: requests<3.0,>=2.8 in /usr/local/lib/python3.6/dist-packages (from vk) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8->vk) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8->vk) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8->vk) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8->vk) (2.10)\n",
            "Building wheels for collected packages: vk\n",
            "  Building wheel for vk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vk: filename=vk-2.0.2-cp36-none-any.whl size=8276 sha256=216f22115ee0d558882204c1af1d38edbe2979d01a626b55226147b5814ce575\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/48/d1/09749ec47d9a30d166122773811f4ccb406f5234f2d84fd29d\n",
            "Successfully built vk\n",
            "Installing collected packages: vk\n",
            "Successfully installed vk-2.0.2\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/41/4a/3360ff3cf2b4a1b9721ac1fbff5f84663f41047d9874b3aa1ac82e862c44/validators-0.18.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (1.15.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators) (4.4.2)\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.18.1\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgV0d-XWNZY7",
        "outputId": "f15477b4-aa83-4532-bfc5-d770729fb0c2"
      },
      "source": [
        "# -*- coding: utf-8 -*- \r\n",
        "#подключаем библиотеки\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import vk\r\n",
        "import requests\r\n",
        "import csv\r\n",
        "import sys\r\n",
        "import validators\r\n",
        "from joblib import Parallel, delayed\r\n",
        "import time\r\n",
        "#основная информация про интернет соединение\r\n",
        "URL = 'https://xn--80afcdbalict6afooklqi5o.xn--p1ai/public/application/cards?SearchString=&Statuses%5B0%5D.Selected=true&Statuses%5B0%5D.Name=%D0%BF%D0%BE%D0%B1%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C+%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0'\r\n",
        "HEADERS = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0', 'accept': '*/*'}\r\n",
        "HOST = 'https://xn--80afcdbalict6afooklqi5o.xn--p1ai/'\r\n",
        "FILE = 'ans.csv'\r\n",
        "def checkNet():   #проверяем работает ли интернет\r\n",
        "    try:\r\n",
        "        requests.get(\"http://www.google.com\",timeout=10)\r\n",
        "        return 1           #работает \r\n",
        "    except requests.ConnectionError:\r\n",
        "        return 0           #не работает\r\n",
        "def aboutli(data):         #перенос строки data которая сделана как список\r\n",
        "    if(str(data).find('</li>')!=-1):  #если есть элемент списка\r\n",
        "        new_data='' #новый ответ\r\n",
        "        str_li=data.find_all('li') #ищем все <li> элемента списка\r\n",
        "        for stroka in str_li: #пробегаемся по ним\r\n",
        "            new_data=new_data+(stroka.text.rstrip().rstrip())+'\\n' #удаляем пробелы и переносим на следующую строчку\r\n",
        "        return new_data     #вернуит обработанный текст\r\n",
        "    else:\r\n",
        "        return data.text    #вернуть изначальный \"обычный\" текст\r\n",
        "def delete_extra_spaces(s):\r\n",
        "    #удаляем префиксы, которые и так написаны в название столбца\r\n",
        "    s=s.replace('Краткое описание','') \r\n",
        "    s=s.replace('Обоснование социальной значимости','')\r\n",
        "    s=s.replace('Цель\\n','')\r\n",
        "    s=s.replace('Задачи\\n','')\r\n",
        "    s=s.replace('География проекта','')\r\n",
        "    s=s.replace('Целевые группы','')\r\n",
        "    st=\"\"\r\n",
        "    #удаляем ненужные пробелы и переносы строк, которые мешают читать данные\r\n",
        "    s=s.strip()\r\n",
        "    for i in range(len(s)):\r\n",
        "        n=s.find(\" \")\r\n",
        "        if n==-1:\r\n",
        "            st=st+s\r\n",
        "            break\r\n",
        "        st=st+s[:n]+' '\r\n",
        "        s=s[n+1:]\r\n",
        "        s=s.lstrip()\r\n",
        "    return st.rstrip() #возвращаем обработанный текст\r\n",
        "def get_html(url,params=None): # делаем запрос на html страничку\r\n",
        "    while(checkNet()==0): #ждем пока будет интернет соедиение\r\n",
        "        print('Отсутсвие интернет соединение. Подключите интернет для продолжения работы')\r\n",
        "    try:\r\n",
        "        r = requests.get(url, headers=HEADERS, params=params,timeout=60) #делаем запрос на страницу максимальный отклик 60секунд\r\n",
        "        return r\r\n",
        "    except:\r\n",
        "        return 'ПРОПУСК' #иначе считаем что она не работает\r\n",
        "def getVariantsOfWords(word): # получаем слово в нормальной кодировке]\r\n",
        "    trans = '[]{}0123456789.,!@\\\"#№;$%^:&?*()\\'\\\\/|' # 'плохие' символы\r\n",
        "    for c in trans:\r\n",
        "        word = word.replace(c, '') # убираем их\r\n",
        "    small = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\r\n",
        "    big = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\r\n",
        "    enSmall = 'abcdefghijklmnopqrstuvwxyz'\r\n",
        "    enBig = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\r\n",
        "    alphaToSmall = {}\r\n",
        "    alphaToBig = {}\r\n",
        "    for i in range(len(big)):\r\n",
        "        alphaToSmall[big[i]] = small[i]\r\n",
        "        alphaToBig[small[i]] = big[i]\r\n",
        "    for i in range(len(enBig)):\r\n",
        "        alphaToSmall[enBig[i]] = enSmall[i]\r\n",
        "        alphaToBig[enSmall[i]] = enBig[i]    \r\n",
        "    res = ''\r\n",
        "    words = []\r\n",
        "    for c in word:\r\n",
        "        if (alphaToSmall.get(c) != None):\r\n",
        "            res = res + alphaToSmall.get(c)\r\n",
        "        else:\r\n",
        "            res = res + c\r\n",
        "    words.append(res)\r\n",
        "    if (len(res) > 0 and alphaToBig.get(res[0]) != None):\r\n",
        "        res = alphaToBig.get(res[0]) + res[1:]\r\n",
        "    words.append(res)\r\n",
        "    res = ''\r\n",
        "    for c in word:\r\n",
        "        if (alphaToBig.get(c) != None):\r\n",
        "            res = res + alphaToBig.get(c)\r\n",
        "        else:\r\n",
        "            res = res + c\r\n",
        "    words.append(res)\r\n",
        "    return words # возвращаем маленькими, с большой, большие\r\n",
        "def get_pages_count(html): #cчитаем количество страниц\r\n",
        "    soup = BeautifulSoup(html, 'html.parser') \r\n",
        "    pagination = soup.find_all('li', class_='pagination__item')\r\n",
        "    if pagination:\r\n",
        "        return int(pagination[-1].get_text())\r\n",
        "    else:\r\n",
        "        return 1\r\n",
        "def NameCheck(string,code1,code2): #Можно ли расшифровать stirng с помощью code1 и code2\r\n",
        "    letters = ['“','…','”','<','>','«','»',chr(9),chr(13),chr(10),'(',')','|',':',' ',chr(34),'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','а','a','б','в','г','д','е','ё','ж','з','и','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я','-','+',',','.','/','[',']','1','2','3','4','5','6','7','8','9','0','#','№','-','—','_','=','}','{','+','!','?','#']\r\n",
        "    try:\r\n",
        "        string=string.encode(code1).decode(code2) #декодируем если можно\r\n",
        "    except:\r\n",
        "        return False # выходим\r\n",
        "    try:\r\n",
        "        string=string.lower() # пытаемся сделать нижний регистр\r\n",
        "    except:\r\n",
        "        string=string\r\n",
        "    for delite_symbole in letters: #удаляем символы\r\n",
        "        string=string.replace(delite_symbole,'')\r\n",
        "    if(len(string)>2): #Если длина остатков больше 2 выходим\r\n",
        "        return False\r\n",
        "    else:\r\n",
        "        return True #все верно\r\n",
        "def get_true_followers(s): #обробатываем 'плохую' строчку с подписчиками\r\n",
        "    s=str(s)\r\n",
        "    s = s.replace(',', '.')\r\n",
        "    s = s.replace(chr(160), ' ')\r\n",
        "    if (s.find('млн подписчиков') != -1):\r\n",
        "        s = s.replace(' млн подписчиков', '')\r\n",
        "        return int(float(s) * 1000000)\r\n",
        "    if (s.find(' тыс. подписчиков') != -1):\r\n",
        "        s = s.replace(' тыс. подписчиков', '')\r\n",
        "        return int(float(s) * 1000)\r\n",
        "    s = s.replace(' подписчиков', '')\r\n",
        "    s = s.replace(' подписчика', '')\r\n",
        "    s = s.replace(' подписчик', '')   \r\n",
        "    try:\r\n",
        "        return int(float(s)) #возвращаем кол-во подписчиков\r\n",
        "    except:\r\n",
        "        return 0\r\n",
        "def get_links_from_page(HTML2): #ищем все ссылки, которые можно найти на сайте\r\n",
        "    links = set()\r\n",
        "    try:\r\n",
        "        soup = BeautifulSoup(HTML2, 'html.parser')\r\n",
        "    except:\r\n",
        "        return links\r\n",
        "    try:\r\n",
        "        for link in soup.find_all('a', href=True):\r\n",
        "            links.add(link['href'])\r\n",
        "    except:\r\n",
        "        return links\r\n",
        "    return links\r\n",
        "def find_number_youtube(index,string): # ищем следующее число после строки\r\n",
        "    KolKov=0\r\n",
        "    stroka=''\r\n",
        "    for i in range(index,len(string)):\r\n",
        "        if(string[i]==chr(34)):\r\n",
        "            KolKov+=1\r\n",
        "        if(KolKov==5):\r\n",
        "            break\r\n",
        "        if(KolKov==4 and string[i]!=chr(34)):\r\n",
        "            stroka=stroka+string[i]\r\n",
        "    return stroka\r\n",
        "def YoutubeFollowers(url): #cчитаем количество подписчиков для ютуба\r\n",
        "    try:\r\n",
        "        HTML_youtube=get_html(url)\r\n",
        "        if(HTML_youtube!='ПРОПУСК'):\r\n",
        "            HTML_youtube=HTML_youtube.text\r\n",
        "            soup_youtube=str(BeautifulSoup(HTML_youtube,'html.parser'))\r\n",
        "            ind = soup_youtube.find('subscriberCountText')\r\n",
        "            if(ind==-1):\r\n",
        "                return 0\r\n",
        "            else:\r\n",
        "                return (find_number_youtube(ind,soup_youtube))\r\n",
        "        else:\r\n",
        "            return 0\r\n",
        "    except:\r\n",
        "        return 0\r\n",
        "def get_social_links(links): #ищем из всех ссылок ссылки на соц. сети\r\n",
        "    prefixes = [['https://www.youtube.com/channel/','http://www.youtube.com/', 'https://www.youtube.com/user/','https://www.youtube.com/c/'], ['https://vk.com/'], ['https://www.instagram.com/']]\r\n",
        "    youtube = []\r\n",
        "    youtube_links=set()\r\n",
        "    vk = []\r\n",
        "    vk_links=set()\r\n",
        "    inst = []\r\n",
        "    inst_links=set()\r\n",
        "    youtube_count=0\r\n",
        "    vk_count=0\r\n",
        "    inst_count=0\r\n",
        "    a=0\r\n",
        "    b=0\r\n",
        "    c=0\r\n",
        "    for link in links:\r\n",
        "        for i in range(len(prefixes)):\r\n",
        "            val = ''\r\n",
        "            for media in prefixes[i]:\r\n",
        "                if (link.find(media) != -1):\r\n",
        "                    val = link\r\n",
        "            if(len(val)!=0):\r\n",
        "                if(i==0):\r\n",
        "                    a=1\r\n",
        "                    youtube_links.add(link)\r\n",
        "                    youtube.append(val)\r\n",
        "                if(i==1):\r\n",
        "                    b=1\r\n",
        "                    vk_links.add(link)\r\n",
        "                    vk.append(val)\r\n",
        "                if(i==2):\r\n",
        "                    c=1\r\n",
        "                    inst_links.add(link)\r\n",
        "                    inst.append(val)\r\n",
        "    if (len(youtube) != 0):\r\n",
        "        val = -1\r\n",
        "        for link in youtube:\r\n",
        "            follow = get_true_followers(YoutubeFollowers(link))\r\n",
        "            if (follow > val):\r\n",
        "                val = follow\r\n",
        "        youtube_count = val\r\n",
        "    vk_count=0\r\n",
        "    inst_count = 0\r\n",
        "    if(a==0):\r\n",
        "        youtube_links='Нет аккаунта'\r\n",
        "    if(b==0):\r\n",
        "        vk_links='Нет аккаунта'\r\n",
        "    if(c==0):\r\n",
        "        inst_links='Нет аккаунта'\r\n",
        "    return youtube_count,vk_count,inst_count,youtube_links,vk_links,inst_links  \r\n",
        "def Found_year(string): #из строчки вытаскиваем цифры (которые дают нам год)\r\n",
        "    year=''\r\n",
        "    for a in string:\r\n",
        "        if(a.isdigit()==True):\r\n",
        "            year=year+a\r\n",
        "    return year\r\n",
        "def VKFollowers(url_name,vk_api): #находим кол-во подписчиков ВК\r\n",
        "    url_name=url_name.rstrip().lstrip()\r\n",
        "    podpisota=0\r\n",
        "    if(url_name.find('public')!=-1):\r\n",
        "        try:\r\n",
        "            id_of_group=(url_name[21:])\r\n",
        "            podpisota = max(podpisota,vk_api.groups.getMembers(group_id=id_of_group, v=5.92)['count'])\r\n",
        "        except:\r\n",
        "            podpisota=0\r\n",
        "    if(url_name.find('club')!=-1):\r\n",
        "        try:\r\n",
        "            id_of_group=(url_name[19:])\r\n",
        "            podpisota = max(podpisota,vk_api.groups.getMembers(group_id=id_of_group, v=5.92)['count'])\r\n",
        "        except:\r\n",
        "            podpisota=podpisota\r\n",
        "    try:\r\n",
        "        id_of_group=url_name[15:]\r\n",
        "        podpisota = max(podpisota,vk_api.groups.getMembers(group_id=id_of_group, v=5.92)['count'])\r\n",
        "    except:\r\n",
        "        podpisota=podpisota\r\n",
        "    return podpisota\r\n",
        "def file_saving(): #cохраняем файл\r\n",
        "    token = \"3fb7074e3fb7074e3fb7074e373fc20ea433fb73fb7074e6000a2640396190c4d381005\"  # Сервисный ключ доступа\r\n",
        "    session = vk.Session(access_token=token)\r\n",
        "    vk_api = vk.API(session)\r\n",
        "    with open(FILE, 'w', newline=\"\",errors='ignore') as file:\r\n",
        "        writer = csv.writer(file, delimiter=',')\r\n",
        "        writer.writerow(['Год конкурса гранта','размер гранта','перечислено фондом','конкурс','регион получателя гранта','направление','название проекта','рейтинг проекта','номер заявки','дата подачи','срок реализации','организация','инн орагнизации','огрн организации','софинансирование','краткое описание','цель','задачи','социальная значимость','география проекта','целевая группа проекта','адрес организации','веб-сайт организации','Работает ли сайт?','title сайта организации','description сайта организации','keywords сайта организации','Cайт принадлежит организации?','Ссылки на соц. сети в Instagramm','Количество подписчиков VK','Ссылки на соц. сети в VK','Количество подписчиков youtube','Ссылки на соц. сети в youtube'])\r\n",
        "        for grant in all_grants:\r\n",
        "            follow=0\r\n",
        "            if(grant['Ссылки на соц. сети в VK']!='Нет аккаунта'):\r\n",
        "                for site in grant['Ссылки на соц. сети в VK']:\r\n",
        "                    follow=max(follow,VKFollowers(site,vk_api)) #cчитаем подписчиков в ВК\r\n",
        "            if(follow==0):\r\n",
        "                follow='Нет аккаунта'\r\n",
        "            #сохраняем в csv файл информацию о конкурсе\r\n",
        "            writer.writerow([grant['Год конкурса гранта'],grant['размер гранта'],grant['перечислено фондом'],grant['конкурс'],grant['регион получателя гранта'],grant['направление'],grant['название проекта'],grant['рейтинг проекта'],grant['номер заявки'],grant['дата подачи'],grant['срок реализации'],grant['организация'],grant['инн орагнизации'],grant['огрн организации'],grant['софинансирование'],grant['краткое описание'],grant['цель'],grant['задачи'],grant['социальная значимость'],grant['география проекта'],grant['целевая группа проекта'],grant['адрес организации'],grant['веб-сайт организации'],grant['Работает ли сайт?'],grant['title сайта организации'],grant['description сайта организации'],grant['keywords сайта организации'],grant['Cайт принадлежит организации?'],grant['Ссылки на соц. сети в Instagramm'],follow,grant['Ссылки на соц. сети в VK'],grant['Количество подписчиков youtube'],grant['Ссылки на соц. сети в youtube']])\r\n",
        "def urlChecker(url): #работает ли сайт?\r\n",
        "    try:\r\n",
        "        if not validators.url(url):\r\n",
        "            return False\r\n",
        "    except:\r\n",
        "        return False\r\n",
        "    try:\r\n",
        "        while(checkNet()==0):\r\n",
        "            print('Отсутсвие интернет соединение. Подключите интернет для продолжения работы')\r\n",
        "        r = requests.head(url,timeout=60)\r\n",
        "        return (r.status_code == 200 or r.status_code==403 or r.status_code==418)\r\n",
        "    except:\r\n",
        "        return False\r\n",
        "def decode(string): #декодируем строку \r\n",
        "    all_code = ['UTF-8','cp1251','latin1'] #возможные виды кодировок\r\n",
        "    chk=0\r\n",
        "    for code_in_all1 in all_code:\r\n",
        "        for code_in_all2 in all_code:\r\n",
        "            if(NameCheck(string,code_in_all1,code_in_all2)==True):\r\n",
        "                code1=code_in_all1\r\n",
        "                code2=code_in_all2\r\n",
        "                chk=1\r\n",
        "                break\r\n",
        "        if(chk==1): #если нашлась кодировка \r\n",
        "            string=string.encode(code1).decode(code2) #декодируем\r\n",
        "            return string,code1,code2\r\n",
        "    return 'У сайта неизвестная кодировка','UTF-8', 'UTF-8'\r\n",
        "def is_site_correct(html_str, all_names,code1,code2): #принадлежит ли сайт организации?\r\n",
        "    allwords=getVariantsOfWords(all_names)\r\n",
        "    for name in allwords:\r\n",
        "        name=name.replace(' ООО ','')\r\n",
        "        name=name.replace(' Ооо ','')\r\n",
        "        name=name.replace(' ооо ','')\r\n",
        "        \r\n",
        "        name=name.replace(' ИП ','')\r\n",
        "        name=name.replace(' Ип ','')\r\n",
        "        name=name.replace(' ип ','')\r\n",
        "        \r\n",
        "        name=name.replace(' АО ','')\r\n",
        "        name=name.replace(' Ао ','')\r\n",
        "        name=name.replace(' ао ','')\r\n",
        "        \r\n",
        "        name=name.replace(' ПАО ','')\r\n",
        "        name=name.replace(' Пао ','')\r\n",
        "        name=name.replace(' пао ','')\r\n",
        "        \r\n",
        "        name=name.replace(' НКО ','')\r\n",
        "        name=name.replace(' Нко ','')\r\n",
        "        name=name.replace(' нко ','')\r\n",
        "        \r\n",
        "        name=name.replace(' ОП ','')\r\n",
        "        name=name.replace(' Оп ','')\r\n",
        "        name=name.replace(' оп ','')\r\n",
        "        \r\n",
        "        name=name.replace(' АССОЦИАЦИЯ ','')\r\n",
        "        name=name.replace(' Ассоциация ','')\r\n",
        "        name=name.replace(' ассоциация ','')\r\n",
        "        \r\n",
        "        name=name.replace(' ОБЩЕСТВО ','')\r\n",
        "        name=name.replace(' Общество ','')\r\n",
        "        name=name.replace(' общество ','')\r\n",
        "        \r\n",
        "        name=name.replace(' КОРПОРАЦИЯ ','')\r\n",
        "        name=name.replace(' Корпорация ','')\r\n",
        "        name=name.replace(' корпорация ','')\r\n",
        "        trans = '[]{}0123456789.,!@\\\"#№;$%^:&?*()\\'\\\\/|' # 'плохие' символы\r\n",
        "        for c in trans:\r\n",
        "            name = name.replace(c, '') # убираем их\r\n",
        "        try:\r\n",
        "            name=name.encode(code2).decode(code1) #кодируем в кодировку сайта\r\n",
        "        except:\r\n",
        "            return False\r\n",
        "        words = name.split() # делим на слова\r\n",
        "        buff = []\r\n",
        "        for word in words:\r\n",
        "            if (len(word) > 3):\r\n",
        "                buff.append(word) # удаляем короткие, добовляем хорошие\r\n",
        "        words = buff\r\n",
        "        try:\r\n",
        "            for word in words:\r\n",
        "                if html_str.find(word) != -1: # ищем\r\n",
        "                    return True\r\n",
        "        except:\r\n",
        "            return False\r\n",
        "    return False\r\n",
        "def process(url_item): #по ссылке ищем всю информацию о гранте\r\n",
        "    if(url_item!=None):\r\n",
        "        html_item = (get_html(url_item))\r\n",
        "        html_item=html_item.text\r\n",
        "        soup_item = BeautifulSoup(html_item, 'html.parser')\r\n",
        "        all_data = soup_item.find_all('li',class_='winner-info__list-item')\r\n",
        "        money=soup_item.find_all('span',class_='circle-bar__info-item-number') \r\n",
        "        project_price=money[0].text                              # размер гранта\r\n",
        "        fond_invest=money[2].text                                # перечислено фондом\r\n",
        "        title=soup_item.find(class_='winner-info__title').text   # название проекта\r\n",
        "        if(title.find('...')!=-1):\r\n",
        "            titel_find = title.split()\r\n",
        "            rm = titel_find[:-1]\r\n",
        "            new_find = ' '.join([str(elem) for elem in rm])\r\n",
        "        else:\r\n",
        "            new_find=title\r\n",
        "        url_new_data='https://xn--80afcdbalict6afooklqi5o.xn--p1ai/public/application/cards?SearchString='+new_find  # ссылка на изначальную страницу                                          \r\n",
        "        url_new_data=url_new_data.strip() \r\n",
        "        html_new_get=(get_html(url_new_data)).text\r\n",
        "        soup_new_get=BeautifulSoup(html_new_get, 'html.parser')\r\n",
        "        try:\r\n",
        "            region=(soup_new_get.find('div',class_='projects__descr')).find('div').text         # регион получателя гранта\r\n",
        "        except:\r\n",
        "            region='Не найдено'\r\n",
        "        try:\r\n",
        "            direction=soup_new_get.find('div',class_='direction').text                          # направление гранта\r\n",
        "        except:\r\n",
        "            direction='Не найдено'\r\n",
        "        contest=all_data[0].find('span',class_='winner-info__list-item-text').text               # конкурс \r\n",
        "        rating = all_data[2].find('span',class_='winner-info__list-item-text').text              # рейтинг проекта\r\n",
        "        number_request = all_data[3].find('span',class_='winner-info__list-item-text').text      # номер заявки\r\n",
        "        date_request = all_data[4].find('span',class_='winner-info__list-item-text').text        # дата подачи\r\n",
        "        date_realization = all_data[5].find('span',class_='winner-info__list-item-text').text    # срок реализации\r\n",
        "        organization = all_data[6].find('span',class_='winner-info__list-item-text').text        # организация\r\n",
        "        inn = all_data[7].find('span',class_='winner-info__list-item-text').text                 # инн орагнизации\r\n",
        "        orgn = all_data[8].find('span',class_='winner-info__list-item-text').text                # огрн орнанизации\r\n",
        "        sofinance = money[1].text     # софинансирование\r\n",
        "        #дополнительная инфа\r\n",
        "        all_dop_data=soup_item.find_all('div',class_='winner__details-box js-ancor-box')\r\n",
        "        winner_summary=aboutli(all_dop_data[0])                                                      # краткое описание\r\n",
        "        winner_aim=aboutli(all_dop_data[1])                                                          # цель\r\n",
        "        winner_tasks=aboutli(all_dop_data[2])                                                        # задачи\r\n",
        "        winner_social=aboutli(all_dop_data[3])                                                       # социальная значимость \r\n",
        "        winner_geo=aboutli(all_dop_data[4])                                                          # география проекта\r\n",
        "        winner_target=aboutli(all_dop_data[5])                                                       # целевая группа проекта\r\n",
        "        winner_contacts=all_dop_data[6]                                                              # контакты организации\r\n",
        "        winner_adress=winner_contacts.find('span',class_='winner__details-contacts-item').text       # адрес организации  \r\n",
        "        try:\r\n",
        "            winner_site=winner_contacts.find('a',class_='winner__details-contacts-item winner__details-contacts-item--link').get('href') # ccылка на веб-сайт\r\n",
        "        except:\r\n",
        "            winner_site='Нет'\r\n",
        "        #################################\r\n",
        "        try:\r\n",
        "            if(winner_site==None or winner_site=='Нет'):\r\n",
        "                site_is_work=False\r\n",
        "            else:\r\n",
        "                if(urlChecker(winner_site)==False):    \r\n",
        "                    if(winner_site[:4]!='http'):\r\n",
        "                        winner_site='http://'+winner_site\r\n",
        "                    site_is_work=urlChecker(winner_site)\r\n",
        "                    if(site_is_work==False):\r\n",
        "                        if(winner_site[:5]=='http:'):\r\n",
        "                            winner_site= 'https'+winner_site[4-(len(winner_site)):]\r\n",
        "                            site_is_work=urlChecker(winner_site)\r\n",
        "                            if(site_is_work==False):\r\n",
        "                                winner_site=winner_site.replace('www.','')\r\n",
        "                                site_is_work=urlChecker(winner_site)\r\n",
        "                else:\r\n",
        "                    site_is_work=True\r\n",
        "        except:\r\n",
        "            site_is_work=False\r\n",
        "            \r\n",
        "        podpis_inst='Нет аккаунта'\r\n",
        "        podpis_vk='Нет аккаунта'\r\n",
        "        podpis_youtube='Нет аккаунта'\r\n",
        "        links_youtube='Нет аккаунта'\r\n",
        "        links_vk='Нет аккаунта'\r\n",
        "        links_inst='Нет аккаунта'\r\n",
        "        #парсинг title, keywords, description           \r\n",
        "        if(site_is_work==True):\r\n",
        "            HTML2=get_html(winner_site)\r\n",
        "            if(HTML2!='ПРОПУСК'):\r\n",
        "                HTML2=HTML2.text\r\n",
        "                soup2=BeautifulSoup(HTML2,'html.parser') \r\n",
        "                try:\r\n",
        "                    title_org_site=(soup2.find('title')).text #title с кодировкой сайта \r\n",
        "                    title_org_site,code1,code2 = decode(title_org_site)\r\n",
        "                    site_correct=is_site_correct(winner_site,organization,code1,code2)\r\n",
        "                except:\r\n",
        "                    code1='UTF-8'\r\n",
        "                    code2='UTF-8'\r\n",
        "                    title_org_site='Не найдено' \r\n",
        "                    site_correct='False'\r\n",
        "                if(title_org_site!='У сайта неизвестная кодировка' and title_org_site!='Не найдено'):\r\n",
        "                    #парсинг description сайта c декодировкой\r\n",
        "                    podpis_youtube,podpis_vk,podpis_inst, youtubes,vks,insts=get_social_links(get_links_from_page(HTML2))\r\n",
        "                    links_youtube=youtubes \r\n",
        "                    links_vk=vks\r\n",
        "                    links_inst=insts\r\n",
        "                    try:\r\n",
        "                        description_org_site=soup2.find(attrs={\"name\":\"description\"})\r\n",
        "                        description_org_site=str(description_org_site)\r\n",
        "                        description_org_site=BeautifulSoup(description_org_site, 'html.parser')\r\n",
        "                        description_org_site=description_org_site.meta['content']\r\n",
        "                        description_org_site=description_org_site.encode(code1).decode(code2)\r\n",
        "                    except:\r\n",
        "                        description_org_site='Не найдено'\r\n",
        "                    if(description_org_site==''):\r\n",
        "                        description_org_site='Отсутсвует'\r\n",
        "                    #парсинг keywords сайта c декодировкой\r\n",
        "                    try:\r\n",
        "                       keywords_org_site=soup2.find(attrs={\"name\":\"keywords\"})\r\n",
        "                       keywords_org_site=str(keywords_org_site)\r\n",
        "                       keywords_org_site = BeautifulSoup(keywords_org_site, 'html.parser')\r\n",
        "                       keywords_org_site=keywords_org_site.meta['content']\r\n",
        "                       keywords_org_site=keywords_org_site.encode(code1).decode(code2)\r\n",
        "                    except:\r\n",
        "                        keywords_org_site='Не найдено'\r\n",
        "                    if(keywords_org_site==''):\r\n",
        "                        keywords_org_site='Отсутсвует'\r\n",
        "                    a=is_site_correct(HTML2,organization,code1,code2)\r\n",
        "                    b=is_site_correct(HTML2,winner_summary,code1,code2)\r\n",
        "                    if(a==True or b==True):\r\n",
        "                        site_correct=True\r\n",
        "                    else:\r\n",
        "                        site_correct=False\r\n",
        "                        \r\n",
        "                #неизвестная кодировка сайта\r\n",
        "                else:\r\n",
        "                    site_correct='False'\r\n",
        "                    title_org_site='Cайт не работает или не существует'\r\n",
        "                    description_org_site='У сайта неизвестная кодировка'\r\n",
        "                    keywords_org_site='У сайта неизвестная кодировка'\r\n",
        "            else:\r\n",
        "                site_correct='Отключение интернета во время получение информации. ERROR.'\r\n",
        "                title_org_site='Отключение интернета во время получение информации. ERROR.'\r\n",
        "                description_org_site='Отключение интернета во время получение информации. ERROR.'\r\n",
        "                keywords_org_site='Отключение интернета во время получение информации. ERROR.'\r\n",
        "        else:\r\n",
        "            site_correct='False'\r\n",
        "            title_org_site='Cайт не работает или не существует'\r\n",
        "            description_org_site='Cайт не работает или не существует'\r\n",
        "            keywords_org_site='Cайт не работает или не существует'\r\n",
        "        #################################\r\n",
        "        if(podpis_youtube==0):\r\n",
        "           podpis_youtube='Нет аккаунта'\r\n",
        "        return({\r\n",
        "            'Год конкурса гранта': Found_year(delete_extra_spaces(contest)),\r\n",
        "            'размер гранта' : delete_extra_spaces(project_price),\r\n",
        "            'перечислено фондом': delete_extra_spaces(fond_invest),\r\n",
        "            'конкурс':delete_extra_spaces(contest),\r\n",
        "            'регион получателя гранта': delete_extra_spaces(region),\r\n",
        "            'направление': delete_extra_spaces(direction),\r\n",
        "            'название проекта':delete_extra_spaces(title),\r\n",
        "            'рейтинг проекта':delete_extra_spaces(rating),\r\n",
        "            'номер заявки':delete_extra_spaces(number_request),\r\n",
        "            'дата подачи':delete_extra_spaces(date_request),\r\n",
        "            'срок реализации':delete_extra_spaces(date_realization),\r\n",
        "            'организация':delete_extra_spaces(organization),\r\n",
        "            'инн орагнизации':delete_extra_spaces(inn),\r\n",
        "            'огрн организации':delete_extra_spaces(orgn),\r\n",
        "            'софинансирование':delete_extra_spaces(sofinance),\r\n",
        "            'краткое описание':delete_extra_spaces(winner_summary),\r\n",
        "            'цель':delete_extra_spaces(winner_aim),\r\n",
        "            'задачи':delete_extra_spaces(winner_tasks),\r\n",
        "            'социальная значимость':delete_extra_spaces(winner_social),\r\n",
        "            'география проекта':delete_extra_spaces(winner_geo),\r\n",
        "            'целевая группа проекта':delete_extra_spaces(winner_target),\r\n",
        "            'адрес организации':delete_extra_spaces(winner_adress),\r\n",
        "            'веб-сайт организации':delete_extra_spaces(winner_site),\r\n",
        "            'Работает ли сайт?':site_is_work,\r\n",
        "            'title сайта организации': title_org_site,           \r\n",
        "            'description сайта организации': description_org_site,\r\n",
        "            'keywords сайта организации': keywords_org_site,\r\n",
        "            'Cайт принадлежит организации?': site_correct,\r\n",
        "            'Ссылки на соц. сети в Instagramm': links_inst, #\r\n",
        "            'Количество подписчиков VK': podpis_vk,\r\n",
        "            'Ссылки на соц. сети в VK': links_vk, #\r\n",
        "            'Количество подписчиков youtube': podpis_youtube,\r\n",
        "            'Ссылки на соц. сети в youtube': links_youtube, # \r\n",
        "        })\r\n",
        "def get_content_from_main(html):\r\n",
        "    soup = BeautifulSoup(html, 'html.parser')\r\n",
        "    cards = soup.find('div',class_='table table--p-present table--projects')\r\n",
        "    items=cards.find_all('a')\r\n",
        "    urls_items=[]\r\n",
        "    global all_grants\r\n",
        "    for item in items:\r\n",
        "        URL = 'https://xn--80afcdbalict6afooklqi5o.xn--p1ai'\r\n",
        "        if(item!=None):   \r\n",
        "            url_item = URL+item.get('href')   # узнаем ссылку\r\n",
        "            url_item = url_item.strip()       # редактируем данные\r\n",
        "            urls_items.append(url_item)       # добавляем все ссылки из сатйа\r\n",
        "    try:\r\n",
        "        all_grants=all_grants+Parallel(n_jobs=8, verbose=100)(delayed(process)(url_item) for url_item in urls_items)       # параллелим процесс с 8 ядрами\r\n",
        "    except:\r\n",
        "        try:                                                                                                              \r\n",
        "            all_grants=all_grants+Parallel(n_jobs=1, verbose=100)(delayed(process)(url_item) for url_item in urls_items)   # если на работает с 8 ядрами запускаем для одного  \r\n",
        "        except:                                                                                                            \r\n",
        "            print('Не хватает мощности')                                                                                   # если не работает с 1 ядром\r\n",
        "def parse(URL): #парсим страницу\r\n",
        "    URL = URL.strip()\r\n",
        "    try: \r\n",
        "        URL_COUNT='https://xn--80afcdbalict6afooklqi5o.xn--p1ai/public/application/cards?SearchString=&Statuses%5B0%5D.Selected=true&Statuses%5B0%5D.Name=%D0%BF%D0%BE%D0%B1%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C+%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0&&page=501'\r\n",
        "        URL_COUNT = URL_COUNT.strip()\r\n",
        "        html = get_html(URL_COUNT) #ищем кол-во 'скрытых' страниц\r\n",
        "        if(html!='ПРОПУСК'):\r\n",
        "            pages_count = get_pages_count(html.text)\r\n",
        "        else:\r\n",
        "            print('Нет доступа к интернету, перезапустите программу когда он появится...')\r\n",
        "            sys.exit()\r\n",
        "    except: #если скрытых страниц нет\r\n",
        "        URL_COUNT='https://xn--80afcdbalict6afooklqi5o.xn--p1ai/public/application/cards?SearchString=&Statuses%5B0%5D.Selected=true&Statuses%5B0%5D.Name=%D0%BF%D0%BE%D0%B1%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C+%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0'\r\n",
        "        URL_COUNT = URL_COUNT.strip()\r\n",
        "        html = get_html(URL_COUNT)\r\n",
        "        if(html!='ПРОПУСК'):\r\n",
        "            pages_count = get_pages_count(html.text) #ищем кол-во открытых страниц\r\n",
        "        else:\r\n",
        "            print('Нет доступа к интернету, перезапустите программу когда он появится...')\r\n",
        "            sys.exit()      \r\n",
        "    for page in range(1,1+1): #pages_count\r\n",
        "        print(f'Парсинг страницы {page} из {pages_count}...')\r\n",
        "        html=get_html(URL, params={'page': page})\r\n",
        "        if(html!='ПРОПУСК'):\r\n",
        "            print('Страница пролучена...')\r\n",
        "            get_content_from_main(html.text)\r\n",
        "        else:\r\n",
        "            print('Страница не найдена...')\r\n",
        "start_time = time.time() #запуск таймера\r\n",
        "all_grants=[]\r\n",
        "parse(URL) #парсим сайт с URL где можно найти только победителев\r\n",
        "file_saving() #сохраняем информацию\r\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) #время выполнения программы"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Парсинг страницы 1 из 794...\n",
            "Страница пролучена...\n",
            "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done   1 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    7.4s\n",
            "[Parallel(n_jobs=8)]: Done   3 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=8)]: Done   4 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=8)]: Done   5 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=8)]: Done   6 out of  20 | elapsed:   10.1s remaining:   23.7s\n",
            "[Parallel(n_jobs=8)]: Done   7 out of  20 | elapsed:   11.1s remaining:   20.5s\n",
            "[Parallel(n_jobs=8)]: Done   8 out of  20 | elapsed:   11.7s remaining:   17.5s\n",
            "[Parallel(n_jobs=8)]: Done   9 out of  20 | elapsed:   12.3s remaining:   15.0s\n",
            "[Parallel(n_jobs=8)]: Done  10 out of  20 | elapsed:   12.7s remaining:   12.7s\n",
            "[Parallel(n_jobs=8)]: Done  11 out of  20 | elapsed:   12.9s remaining:   10.6s\n",
            "[Parallel(n_jobs=8)]: Done  12 out of  20 | elapsed:   13.0s remaining:    8.7s\n",
            "[Parallel(n_jobs=8)]: Done  13 out of  20 | elapsed:   13.4s remaining:    7.2s\n",
            "[Parallel(n_jobs=8)]: Done  14 out of  20 | elapsed:   13.8s remaining:    5.9s\n",
            "[Parallel(n_jobs=8)]: Done  15 out of  20 | elapsed:   15.9s remaining:    5.3s\n",
            "[Parallel(n_jobs=8)]: Done  16 out of  20 | elapsed:   16.9s remaining:    4.2s\n",
            "[Parallel(n_jobs=8)]: Done  17 out of  20 | elapsed:   17.5s remaining:    3.1s\n",
            "[Parallel(n_jobs=8)]: Done  18 out of  20 | elapsed:   18.3s remaining:    2.0s\n",
            "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:   40.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:   40.1s finished\n",
            "--- 46.38327646255493 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}